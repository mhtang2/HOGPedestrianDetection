{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.io import read_image \n",
    "import torchvision\n",
    "import PIL\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_equal_samples(data, label_column='label'):\n",
    "    \"\"\" Select equal # of pos/neg samples \"\"\"\n",
    "    positive_samples = data[data[label_column] == True]\n",
    "    negative_samples = data[data[label_column] == False]\n",
    "\n",
    "    # Determine the number of samples to select (minimum of positive and negative samples)\n",
    "    num_samples = min(len(positive_samples), len(negative_samples))\n",
    "\n",
    "    # Randomly select equal number of positive and negative samples without replacement\n",
    "    selected_positive_samples = positive_samples.sample(n=num_samples, random_state=42, replace=False)\n",
    "    selected_negative_samples = negative_samples.sample(n=num_samples, random_state=42, replace=False)\n",
    "\n",
    "    # Concatenate the selected samples\n",
    "    selected_samples = pd.concat([selected_positive_samples, selected_negative_samples])\n",
    "\n",
    "    return selected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG_PedestrianClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, labels_csv, root_dir, balance_labels=True, transform=None):\n",
    "        self.labels = pd.read_csv(labels_csv)\n",
    "        if balance_labels:\n",
    "            self.labels = select_equal_samples(self.labels)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {True: 1, False: 0}\n",
    "        # self.image_files = sorted(self.labels['image'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        image_name = row['image'] + \".npy\"\n",
    "        image_name = os.path.join(self.root_dir, image_name) \n",
    "        image = np.load(image_name)\n",
    "        image = image.reshape(image.shape[0], image.shape[1], -1) # flatten last 3 dimensions\n",
    "        image = torch.from_numpy(image.transpose((2,0,1))).float()\n",
    "        label = self.class_to_idx[row['label']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset len:  5174\n",
      "torch.Size([36, 23, 35])\n"
     ]
    }
   ],
   "source": [
    "dataset = HOG_PedestrianClassificationDataset(\"./processed_labels.csv\",\"./road-waymo/hog-fd/\", balance_labels=True, transform = None)\n",
    "print(f\"Dataset len:  {len(dataset)}\")\n",
    "\n",
    "sample = dataset[5000]\n",
    "print(sample[0].shape)\n",
    "# to_pil_image(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Use random_split to split the dataset into training and testing sets.\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 10 # TODO: change num_epoch to like 10 or 15 or something\n",
    "def train(model, train_dataloader, val_dataloader, save_path):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer =  torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in tqdm(range(num_epoch), desc=\"epochs\"):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "        for train_data in tqdm(train_dataloader, desc=\"training 1 epoch\"):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = train_data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # break # TODO: delete this\n",
    "        \n",
    "        # Validate\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        all_predicted = torch.tensor([]).to(device)\n",
    "        all_labels = torch.tensor([]).to(device)\n",
    "        all_pred_probs = torch.tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = model(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_predicted = torch.cat((all_predicted, predicted))\n",
    "                all_labels = torch.cat((all_labels, labels))\n",
    "                all_pred_probs = torch.cat((all_pred_probs, outputs.data))\n",
    "                # break # TODO: delete this\n",
    "\n",
    "\n",
    "        conf_matrix = confusion_matrix(all_labels.cpu(), all_predicted.cpu())\n",
    "        print(\"Val confusion matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(f\"{conf_matrix[0].sum().item()} frames don't have pedestrian, {conf_matrix[1].sum().item()} frames have pedestrian\")\n",
    "        \n",
    "        tp = conf_matrix[0,0]\n",
    "        fp = conf_matrix[1,0]\n",
    "        fn = conf_matrix[0,1]\n",
    "        print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "        print(f\"Precision: {tp/(tp+fp)}, Recall: {tp/(tp+fn)}\")\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print(f\"Epoch {epoch}: Saving model to {save_path} with val accuracy {accuracy}%\")\n",
    "            torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load Saved Model\n",
      "[Errno 2] No such file or directory: './checkpoints/resnet18_HOG_fd'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:52<00:00,  1.25it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.67it/s]\n",
      "epochs:  10%|█         | 1/10 [00:54<08:12, 54.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[245  22]\n",
      " [ 34 216]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 245, FP: 34, FN: 22\n",
      "Precision: 0.8781362007168458, Recall: 0.9176029962546817\n",
      "Epoch 0: Saving model to ./checkpoints/resnet18_HOG_fd with val accuracy 89.16827852998065%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:49<00:00,  1.31it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.64it/s]\n",
      "epochs:  20%|██        | 2/10 [01:47<07:07, 53.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[256  11]\n",
      " [  8 242]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 256, FP: 8, FN: 11\n",
      "Precision: 0.9696969696969697, Recall: 0.9588014981273408\n",
      "Epoch 1: Saving model to ./checkpoints/resnet18_HOG_fd with val accuracy 96.32495164410058%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:50<00:00,  1.29it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.91it/s]\n",
      "epochs:  30%|███       | 3/10 [02:39<06:11, 53.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[262   5]\n",
      " [ 10 240]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 262, FP: 10, FN: 5\n",
      "Precision: 0.9632352941176471, Recall: 0.9812734082397003\n",
      "Epoch 2: Saving model to ./checkpoints/resnet18_HOG_fd with val accuracy 97.09864603481624%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:53<00:00,  1.21it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[257  10]\n",
      " [  4 246]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 257, FP: 4, FN: 10\n",
      "Precision: 0.9846743295019157, Recall: 0.9625468164794008\n",
      "Epoch 3: Saving model to ./checkpoints/resnet18_HOG_fd with val accuracy 97.29206963249517%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:54<00:00,  1.20it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[259   8]\n",
      " [  3 247]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 259, FP: 3, FN: 8\n",
      "Precision: 0.9885496183206107, Recall: 0.9700374531835206\n",
      "Epoch 4: Saving model to ./checkpoints/resnet18_HOG_fd with val accuracy 97.87234042553192%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:51<00:00,  1.26it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.94it/s]\n",
      "epochs:  60%|██████    | 6/10 [05:27<03:39, 54.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[258   9]\n",
      " [  3 247]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 258, FP: 3, FN: 9\n",
      "Precision: 0.9885057471264368, Recall: 0.9662921348314607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:49<00:00,  1.32it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.67it/s]\n",
      "epochs:  70%|███████   | 7/10 [06:19<02:41, 53.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[257  10]\n",
      " [  3 247]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 257, FP: 3, FN: 10\n",
      "Precision: 0.9884615384615385, Recall: 0.9625468164794008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:49<00:00,  1.33it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n",
      "epochs:  80%|████████  | 8/10 [07:10<01:46, 53.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[259   8]\n",
      " [  3 247]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 259, FP: 3, FN: 8\n",
      "Precision: 0.9885496183206107, Recall: 0.9700374531835206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:49<00:00,  1.33it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.88it/s]\n",
      "epochs:  90%|█████████ | 9/10 [08:02<00:52, 52.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[258   9]\n",
      " [  2 248]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 258, FP: 2, FN: 9\n",
      "Precision: 0.9923076923076923, Recall: 0.9662921348314607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training 1 epoch: 100%|██████████| 65/65 [00:49<00:00,  1.33it/s]\n",
      "Validating: 100%|██████████| 9/9 [00:02<00:00,  3.82it/s]\n",
      "epochs: 100%|██████████| 10/10 [08:53<00:00, 53.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val confusion matrix:\n",
      "[[259   8]\n",
      " [  3 247]]\n",
      "267 frames don't have pedestrian, 250 frames have pedestrian\n",
      "TP: 259, FP: 3, FN: 8\n",
      "Precision: 0.9885496183206107, Recall: 0.9700374531835206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./checkpoints/resnet18_HOG_fd\"\n",
    "\n",
    "model = torchvision.models.resnet18(weights = None)\n",
    "model.conv1 = torch.nn.Conv2d(2*2*9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(\"Successfully Loaded Saved Model\")\n",
    "except Exception as error:\n",
    "    print(\"Failed to load Saved Model\")\n",
    "    print(error)\n",
    "model.to(device)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, save_path, test_type):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    all_predicted = torch.tensor([]).to(device)\n",
    "    all_labels = torch.tensor([]).to(device)\n",
    "    all_pred_probs = torch.tensor([]).to(device)\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dataloader, desc=\"testing\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predicted = torch.cat((all_predicted, predicted))\n",
    "            all_labels = torch.cat((all_labels, labels))\n",
    "            all_pred_probs = torch.cat((all_pred_probs, outputs.data))\n",
    "            # break # TODO: remove this\n",
    "\n",
    "    # Let's say y_true is your true binary labels and y_pred_probs is the predicted probabilities for the positive class\n",
    "    # You should replace them with your actual variables\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(all_labels.cpu(), all_pred_probs[:,1].cpu(), pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"ROC curve area:\", roc_auc)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels.cpu(), all_predicted.cpu())\n",
    "    print(conf_matrix)\n",
    "    print(f\"{conf_matrix[0].sum().item()} frames don't have pedestrian, {conf_matrix[1].sum().item()} frames have pedestrian\")\n",
    "    tp = conf_matrix[0,0]\n",
    "    fp = conf_matrix[1,0]\n",
    "    fn = conf_matrix[0,1]\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "    print(f\"Precision: {tp/(tp+fp)}, Recall: {tp/(tp+fn)}\")\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy for {save_path}: {accuracy}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Loaded Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|██████████| 9/9 [00:02<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve area: 0.99606393129771\n",
      "[[251   5]\n",
      " [  4 258]]\n",
      "256 frames don't have pedestrian, 262 frames have pedestrian\n",
      "TP: 251, FP: 4, FN: 5\n",
      "Precision: 0.984313725490196, Recall: 0.98046875\n",
      "Accuracy for ./checkpoints/resnet18_HOG_fd: 98.26254826254826\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(weights = None)\n",
    "model.conv1 = torch.nn.Conv2d(2*2*9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(\"Successfully Loaded Saved Model\")\n",
    "except Exception as error:\n",
    "    print(\"Failed to load Saved Model\")\n",
    "    print(error)\n",
    "model.to(device)\n",
    "\n",
    "test(model, test_dataloader, save_path, \"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
